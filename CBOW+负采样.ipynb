{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0c9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 步骤1：文本清洗与分词\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca414eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 语料库路径\n",
    "corpus_path = r'C:\\Users\\20030421a\\Desktop\\text8.txt'\n",
    "\n",
    "# 参数设置\n",
    "min_count = 5  # 低频词过滤阈值\n",
    "\n",
    "# 初始化变量\n",
    "word_freq = {}  # 统计每个词的频率\n",
    "id2word = []    # 索引到词的映射，如 [UNK, \"he\", \"is\", ...]\n",
    "word2id = {}    # 词到索引的映射，如 {\"he\":1, \"is\":2, ...}\n",
    "\n",
    "def preprocess_text(file_path):\n",
    "    \"\"\"\n",
    "    读取语料库文件，清洗文本并分词\n",
    "    :param file_path: 语料库文件路径\n",
    "    :return: 分词后的词列表\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()  # 读取文件内容并去除首尾空白\n",
    "    # 分词（英文按空格分割）\n",
    "    tokenized_text = text.split()\n",
    "    # 选取前 1% 的数据\n",
    "    tokenized_text = tokenized_text[:int(len(tokenized_text) * 0.01)]\n",
    "    # 转为小写\n",
    "    tokenized_text = [word.lower() for word in tokenized_text]\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5126f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 记录代码开始执行的时间\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ca5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的词列表（前10个）：['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，获取分词后的词列表\n",
    "tokenized_text = preprocess_text(corpus_path)#列表\n",
    "print(f\"分词后的词列表（前10个）：{tokenized_text[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b8fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 步骤2：统计词频并过滤低频词\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8475f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_freq(tokenized_text):\n",
    "    \"\"\"\n",
    "    统计词频并过滤低频词\n",
    "    :param tokenized_text: 分词后的词列表\n",
    "    :return: 过滤后的词频字典\n",
    "    \"\"\"\n",
    "    word_freq = {}\n",
    "    for word in tokenized_text:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    # 过滤低频词\n",
    "    word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_count}\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7e089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词频字典（前10项）：[('anarchism', 104), ('originated', 9), ('as', 1414), ('a', 3074), ('term', 68), ('of', 6071), ('abuse', 14), ('first', 237), ('used', 190), ('against', 77)]\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，获取词频字典\n",
    "word_freq = build_word_freq(tokenized_text)\n",
    "print(f\"词频字典（前10项）：{list(word_freq.items())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b3bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 步骤3：构建词汇表\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59057da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_freq):\n",
    "    \"\"\"\n",
    "    构建词汇表（词到ID的映射和ID到词的映射）\n",
    "    :param word_freq: 词频字典\n",
    "    :return: id2word, word2id\n",
    "    \"\"\"\n",
    "    id2word = [\"[UNK]\"]  # ID=0 保留给未知词\n",
    "    word2id = {\"[UNK]\": 0}\n",
    "    idx = 1\n",
    "    for word, freq in word_freq.items():\n",
    "        word2id[word] = idx\n",
    "        id2word.append(word)\n",
    "        idx += 1\n",
    "    return id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dbeacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用函数，获取词汇表\n",
    "id2word, word2id = build_vocab(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad876584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#高频词采样\n",
    "def subsample_words(tokenized_text, t=1e-5):\n",
    "    new_text = []\n",
    "    for word in tokenized_text:\n",
    "        freq = word_freq.get(word, 0)\n",
    "        prob = 1 - np.sqrt(t / (freq + 1e-5))  # 防止除以0\n",
    "        if np.random.rand() > prob:  # 以概率prob保留该词\n",
    "            new_text.append(word)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0fc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5         # 最大窗口\n",
    "train_data = []         # 存储 (中心词id, [上下文id列表])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2258d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_text)):\n",
    "    center_word = tokenized_text[i]\n",
    "    center_id = word2id.get(center_word, 0)  # 未知词映射为0\n",
    "    if center_id == 0:  # 跳过未知词作为中心词\n",
    "        continue\n",
    "    # 随机生成当前窗口大小\n",
    "    curr_window = np.random.randint(1, window_size+1)\n",
    "    context_ids = []\n",
    "    # 取左右各curr_window个词\n",
    "    for j in range(max(0, i - curr_window), min(len(tokenized_text), i + curr_window + 1)):\n",
    "        if j != i:  # 排除中心词自己\n",
    "            context_word = tokenized_text[j]\n",
    "            context_id = word2id.get(context_word, 0)\n",
    "            if context_id != 0:  # 忽略未知词\n",
    "                context_ids.append(context_id)\n",
    "    if len(context_ids) > 0:\n",
    "        train_data.append( (center_id, context_ids) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31bd7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个上下文词单独拆分为正样本\n",
    "expanded_train_data = []\n",
    "for center_id, context_ids in train_data:\n",
    "    for context_id in context_ids:\n",
    "        expanded_train_data.append( (center_id, context_id) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20a3a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据词频计算负采样概率分布（幂律分布，论文中建议使用3/4次方）\n",
    "word_counts = np.array([word_freq.get(word, 0) for word in id2word], dtype=np.float32)\n",
    "word_counts[0] = 0  # 排除未知词[UNK]\n",
    "word_probs = word_counts ** 0.75\n",
    "word_probs /= word_probs.sum()  # 归一化为概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce3ca898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(context_id, num_neg_samples):\n",
    "    \"\"\"\n",
    "    根据概率分布生成负样本（排除正样本词）\n",
    "    :param context_id: 正样本的上下文词ID（避免采样到该词）\n",
    "    :param num_neg_samples: 负样本数量\n",
    "    :return: 负样本ID列表\n",
    "    \"\"\"\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < num_neg_samples:\n",
    "        # 按分布采样，排除context_id\n",
    "        sampled_id = np.random.choice(len(word_probs), p=word_probs)\n",
    "        if sampled_id != context_id and sampled_id != 0:  # 排除未知词和正样本\n",
    "            neg_samples.append(sampled_id)\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c135e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW + Negative Sampling\n",
    "vocab_size = len(id2word)   # 词汇表大小\n",
    "embedding_dim = 100         # 词向量维度\n",
    "neg_samples = 5             # 每个正样本对应的负样本数\n",
    "# 输入向量（上下文词）和输出向量（中心词）\n",
    "input_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01  # 上下文词向量矩阵\n",
    "output_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01 # 中心词向量矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fefe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_loss(center_id, context_id, neg_samples):\n",
    "    \"\"\"\n",
    "    计算CBOW模型的负采样损失\n",
    "    :param center_id: 中心词ID\n",
    "    :param context_id: 上下文词ID（正样本）\n",
    "    :param neg_samples: 负样本ID列表\n",
    "    :return: 损失值，中心词梯度，上下文词梯度，负样本梯度\n",
    "    \"\"\"\n",
    "    # 获取输入向量（上下文词向量）\n",
    "    h = input_embeddings[context_id]  # [embedding_dim, ]\n",
    "    \n",
    "    # 正样本得分（中心词向量）\n",
    "    pos_out = output_embeddings[center_id]  # [embedding_dim, ]\n",
    "    pos_score = 1 / (1 + np.exp(-np.dot(pos_out, h)))  # Sigmoid(点积)\n",
    "    pos_loss = -np.log(pos_score)  # 正样本损失\n",
    "    \n",
    "    # 负样本得分\n",
    "    neg_loss = 0\n",
    "    neg_grads = []\n",
    "    for neg_id in neg_samples:\n",
    "        neg_out = output_embeddings[neg_id]\n",
    "        neg_score = 1 / (1 + np.exp(np.dot(neg_out, h)))  # Sigmoid(-点积)\n",
    "        neg_loss += -np.log(neg_score)\n",
    "        neg_grads.append(neg_out * (neg_score - 1))  # 负样本梯度\n",
    "    \n",
    "    total_loss = pos_loss + neg_loss\n",
    "    \n",
    "    # 计算梯度\n",
    "    # 正样本梯度\n",
    "    grad_pos_out = (pos_score - 1) * h  # dL/d(pos_out)\n",
    "    grad_h_pos = (pos_score - 1) * pos_out  # dL/dh（来自正样本）\n",
    "    \n",
    "    # 负样本梯度（累加到h的梯度）\n",
    "    grad_h_neg = np.zeros_like(h)\n",
    "    for neg_grad in neg_grads:\n",
    "        grad_h_neg += neg_grad\n",
    "    grad_h = grad_h_pos + grad_h_neg  # 总梯度\n",
    "    \n",
    "    return total_loss, grad_h, grad_pos_out, neg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2352b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "num_epochs = 5\n",
    "neg_samples_per_pos = 5  # 每个正样本对应的负样本数\n",
    "batch_size = 128  # 小批量训练（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3057f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Progress: 10.0% | Current Loss: 59.7244\n",
      "Epoch 0 | Progress: 20.0% | Current Loss: 73.4698\n",
      "Epoch 0 | Progress: 30.0% | Current Loss: 79.3511\n",
      "Epoch 0 | Progress: 40.0% | Current Loss: 82.4859\n",
      "Epoch 0 | Progress: 50.0% | Current Loss: 84.3998\n",
      "Epoch 0 | Progress: 60.0% | Current Loss: 85.6818\n",
      "Epoch 0 | Progress: 70.0% | Current Loss: 86.5989\n",
      "Epoch 0 | Progress: 80.0% | Current Loss: 87.2870\n",
      "Epoch 0 | Progress: 90.0% | Current Loss: 87.8221\n",
      "Epoch 0 | Progress: 100.0% | Current Loss: 88.2503\n",
      "Epoch 0 Completed | Avg Loss: 88.2503 | Learning Rate: 0.019000\n",
      "============================================================\n",
      "Epoch 1 | Progress: 10.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 20.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 30.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 40.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 50.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 60.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 70.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 80.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 90.0% | Current Loss: 92.1034\n",
      "Epoch 1 | Progress: 100.0% | Current Loss: 92.1034\n",
      "Epoch 1 Completed | Avg Loss: 92.1034 | Learning Rate: 0.018050\n",
      "============================================================\n",
      "Epoch 2 | Progress: 10.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 20.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 30.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 40.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 50.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 60.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 70.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 80.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 90.0% | Current Loss: 92.1034\n",
      "Epoch 2 | Progress: 100.0% | Current Loss: 92.1034\n",
      "Epoch 2 Completed | Avg Loss: 92.1034 | Learning Rate: 0.017147\n",
      "============================================================\n",
      "Epoch 3 | Progress: 10.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 20.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 30.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 40.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 50.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 60.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 70.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 80.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 90.0% | Current Loss: 92.1034\n",
      "Epoch 3 | Progress: 100.0% | Current Loss: 92.1034\n",
      "Epoch 3 Completed | Avg Loss: 92.1034 | Learning Rate: 0.016290\n",
      "============================================================\n",
      "Epoch 4 | Progress: 10.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 20.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 30.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 40.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 50.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 60.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 70.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 80.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 90.0% | Current Loss: 92.1034\n",
      "Epoch 4 | Progress: 100.0% | Current Loss: 92.1034\n",
      "Epoch 4 Completed | Avg Loss: 92.1034 | Learning Rate: 0.015476\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 数值稳定函数定义\n",
    "def safe_sigmoid(x):\n",
    "    \"\"\"数值稳定的sigmoid函数\"\"\"\n",
    "    x_clipped = np.clip(x, -100, 100)  # 防止exp溢出\n",
    "    return 1 / (1 + np.exp(-x_clipped))\n",
    "\n",
    "def safe_log(x, eps=1e-8):\n",
    "    \"\"\"数值稳定的对数函数\"\"\"\n",
    "    return np.log(np.clip(x, eps, 1.0 - eps))\n",
    "\n",
    "# 修改后的损失函数\n",
    "def cbow_loss(center_id, context_id, neg_samples):\n",
    "    # 获取输入向量（上下文词向量）\n",
    "    h = input_embeddings[context_id]  # [embedding_dim, ]\n",
    "    \n",
    "    # 正样本得分（中心词向量）\n",
    "    pos_out = output_embeddings[center_id]  # [embedding_dim, ]\n",
    "    dot_pos = np.dot(pos_out, h)\n",
    "    pos_score = safe_sigmoid(dot_pos)  # 使用安全sigmoid\n",
    "    pos_loss = -safe_log(pos_score)     # 使用安全log\n",
    "    \n",
    "    # 负样本得分\n",
    "    neg_loss = 0\n",
    "    neg_grads = []\n",
    "    for neg_id in neg_samples:\n",
    "        neg_out = output_embeddings[neg_id]\n",
    "        dot_neg = np.dot(neg_out, h)\n",
    "        neg_score = safe_sigmoid(-dot_neg)  # 注意负号\n",
    "        neg_loss += -safe_log(neg_score)\n",
    "        # 计算梯度时保持数值稳定\n",
    "        grad_factor = (neg_score - 1) * h\n",
    "        grad_factor = np.clip(grad_factor, -5.0, 5.0)  # 梯度裁剪\n",
    "        neg_grads.append(grad_factor)\n",
    "    \n",
    "    total_loss = pos_loss + neg_loss\n",
    "    \n",
    "    # 计算梯度（加入梯度裁剪）\n",
    "    grad_pos_out = np.clip((pos_score - 1) * h, -5.0, 5.0)\n",
    "    grad_h_pos = np.clip((pos_score - 1) * pos_out, -5.0, 5.0)\n",
    "    \n",
    "    grad_h_neg = np.zeros_like(h)\n",
    "    for grad in neg_grads:\n",
    "        grad_h_neg += grad\n",
    "    grad_h = grad_h_pos + grad_h_neg\n",
    "    \n",
    "    return total_loss, grad_h, grad_pos_out, neg_grads\n",
    "\n",
    "# 训练循环修改版\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(expanded_train_data)\n",
    "    total_loss = 0\n",
    "    samples_processed = 0\n",
    "    \n",
    "    # 加入进度打印\n",
    "    print_interval = max(1, len(expanded_train_data) // 10)\n",
    "    \n",
    "    for idx, (center_id, context_id) in enumerate(expanded_train_data):\n",
    "        # 生成负样本\n",
    "        neg_samples = get_negative_samples(context_id, neg_samples_per_pos)\n",
    "        \n",
    "        # 计算损失和梯度\n",
    "        loss, grad_h, grad_pos_out, neg_grads = cbow_loss(center_id, context_id, neg_samples)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        grad_h = np.clip(grad_h, -5.0, 5.0)\n",
    "        grad_pos_out = np.clip(grad_pos_out, -5.0, 5.0)\n",
    "        neg_grads = [np.clip(g, -5.0, 5.0) for g in neg_grads]\n",
    "        \n",
    "        # 更新参数\n",
    "        input_embeddings[context_id] -= learning_rate * grad_h\n",
    "        output_embeddings[center_id] -= learning_rate * grad_pos_out\n",
    "        for i, neg_id in enumerate(neg_samples):\n",
    "            output_embeddings[neg_id] -= learning_rate * neg_grads[i]\n",
    "        \n",
    "        # 进度打印\n",
    "        samples_processed += 1\n",
    "        if (idx + 1) % print_interval == 0:\n",
    "            avg_loss = total_loss / samples_processed\n",
    "            print(f\"Epoch {epoch} | Progress: {(idx+1)/len(expanded_train_data)*100:.1f}% | Current Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 学习率衰减调整\n",
    "    learning_rate = max(0.0001, learning_rate * 0.95)  # 更激进的衰减\n",
    "    \n",
    "    # 每个epoch结束后打印\n",
    "    avg_loss = total_loss / len(expanded_train_data)\n",
    "    print(f\"Epoch {epoch} Completed | Avg Loss: {avg_loss:.4f} | Learning Rate: {learning_rate:.6f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 每个epoch保存检查点\n",
    "    np.save(f\"word_vectors_epoch{epoch}.npy\", input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03454255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存输入向量（上下文词向量）或输出向量（通常使用输入向量）\n",
    "np.save(\"word_vectors.npy\", input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87247c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录代码结束执行的时间\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "717975f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1739369273.8297653"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "564a6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与 'king' 最相似的词：\n",
      "indeed: 0.922\n",
      "hector: 0.906\n",
      "origin: 0.903\n",
      "rich: 0.890\n",
      "differences: 0.885\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def find_most_similar(word, top_k=5):\n",
    "    if word not in word2id:\n",
    "        return []\n",
    "    word_id = word2id[word]\n",
    "    word_vec = input_embeddings[word_id]\n",
    "    similarities = []\n",
    "    for idx, w in enumerate(id2word):\n",
    "        if idx == 0 or idx == word_id:\n",
    "            continue\n",
    "        sim = cosine_similarity(word_vec, input_embeddings[idx])\n",
    "        similarities.append( (w, sim) )\n",
    "    similarities.sort(key=lambda x: -x[1])\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# 测试相似词\n",
    "test_word = \"king\"\n",
    "similar_words = find_most_similar(test_word)\n",
    "print(f\"与 '{test_word}' 最相似的词：\")\n",
    "for word, sim in similar_words:\n",
    "    print(f\"{word}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21e46b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类比任务结果：man : woman :: king : ?\n",
      "verbal: 0.902\n",
      "africans: 0.875\n",
      "leader: 0.873\n"
     ]
    }
   ],
   "source": [
    "def analogy(a, b, c, top_k=3):\n",
    "    # 计算 vec(a) - vec(b) + vec(c)\n",
    "    a_id = word2id.get(a, 0)\n",
    "    b_id = word2id.get(b, 0)\n",
    "    c_id = word2id.get(c, 0)\n",
    "    if a_id == 0 or b_id == 0 or c_id == 0:\n",
    "        return []\n",
    "    vec = input_embeddings[a_id] - input_embeddings[b_id] + input_embeddings[c_id]\n",
    "    similarities = []\n",
    "    for idx, w in enumerate(id2word):\n",
    "        if idx == 0 or idx in [a_id, b_id, c_id]:\n",
    "            continue\n",
    "        sim = cosine_similarity(vec, input_embeddings[idx])\n",
    "        similarities.append( (w, sim) )\n",
    "    similarities.sort(key=lambda x: -x[1])\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# 测试类比任务（king - man + woman = queen）\n",
    "result = analogy(\"man\", \"woman\", \"king\")\n",
    "print(\"类比任务结果：man : woman :: king : ?\")\n",
    "for word, sim in result:\n",
    "    print(f\"{word}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2580a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
